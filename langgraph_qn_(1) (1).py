# -*- coding: utf-8 -*-
"""Langgraph_QN (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PAePpKO-l5M83JsEZce4e7f_g4RRjIkJ
"""

!pip install -r requirements.txt

!pip install langchain langchain-community langgraph transformers torch sentence-transformers pypdf pymupdf python-docx
# !pip install faiss-gpu-cu12
!pip install langchain-groq
!pip install langchain_ollama
!pip install chromadb


from google.colab import userdata
import numpy as np
import pandas as pd

from pathlib import Path
import os
from langchain_community.document_loaders import PyMuPDFLoader, TextLoader
from typing import List
import pandas as pd
from langchain.schema import Document

def transcript_chunks_to_docs(parquet_path):
    df = pd.read_parquet(parquet_path)
    return [Document(page_content=row["text"], metadata = {
        "source": parquet_path,
        "start_time": row.get("start_ts", None),
        "end_time": row.get("end_ts", None),
        "chunk_idx": row.get("chunk_idx", None)
    }) for _, row in df.iterrows()]


class DocumentLoader:
  def __init__(self):
    self.supported_extensions = {'.pdf', '.txt', '.parquet'}


  def _valid_file(self, loaded_file):
    if loaded_file in self.supported_extensions:
      return True
    return False

  def _load_pdf(self, file_path):
    loader = PyMuPDFLoader(file_path)
    docs = loader.load()
    for d in docs:
      d.metadata["source"] = file_path
    return docs

  def _load_text(self, file_path):
    loader = TextLoader(file_path)
    docs = loader.load()
    for d in docs:
      d.metadata["source"] = file_path
    return docs


  def load_document(self, file_path):
    """
    Loads documents and stores them as lists
    """
    if Path(file_path).suffix == ".parquet":
      return transcript_chunks_to_docs(file_path)
    elif Path(file_path).suffix == ".pdf":
      loader = PyMuPDFLoader(file_path)
      return loader.load()

    else:
      loader = TextLoader(file_path)
      return loader.load()

from langchain_text_splitters import RecursiveCharacterTextSplitter

class TextChunker:
  def __init__(self, chunk_size=100, chunk_overlap=0):
    self.chunk_size = chunk_size
    self.chunk_overlap = chunk_overlap

    self.recursive_splitter =  RecursiveCharacterTextSplitter(chunk_size = self.chunk_size, chunk_overlap= self.chunk_overlap, separators=["\n\n", "\n", "."])

  def chunk_documents(self, documents, strategy = 'rec'):
    if strategy=='rec':
      return self.recursive_splitter.split_documents(documents)

file_path = '/content/MCAKCA032-PRINCIPALES OF SOFT COMPUTING-SN SIVNANDAM AND DEEPA SN (1).pdf'
#file_path = '/content/lecture_transcript.txt'
#file_path = '/content/ana-bell-v1.parquet'

doc_loader = DocumentLoader()
text_chunker = TextChunker(chunk_size=1000, chunk_overlap=100)
documents = doc_loader.load_document(file_path)
print(f"Number of documents loaded: {len(documents)}")
print(text_chunker.chunk_documents(documents)[0])

from sentence_transformers import SentenceTransformer


class EmbeddingModel:
  def __init__(self, model_name = 'sentence-transformers/all-MiniLM-L6-v2'):
    self.model_name = model_name
    self.model = SentenceTransformer(self.model_name)

  def embed_documents(self, texts):
    return self.model.encode(texts,convert_to_tensor=False).tolist()


  def embed_query(self, text):
    return self.model.encode(text,convert_to_tensor=False).tolist()


# model = SentenceTransformer("")
# embeddings =

import os
import numpy as np
from langchain.vectorstores import Chroma
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain.schema import Document

def transcript_chunks_to_docs(parquet_path):
    df = pd.read_parquet(parquet_path)
    return [Document(page_content=row["text"], metadata = {
        "source": parquet_path,
        "start_time": row.get("start_ts", None),
        "end_time": row.get("end_ts", None),
        "chunk_idx": row.get("chunk_idx", None)
    }) for _, row in df.iterrows()]

class VectorStore:
    def __init__(self, embedding_model, persist_dir="/content/drive/MyDrive/chroma_db"):
        self.embedding_model = embedding_model
        self.persist_dir = persist_dir
        self.vectorstore = None


    def _embedding_function(self, x):
        if isinstance(x, str):
          return self.embedding_model.embed_query(x)
        return self.embedding_model.embed_docs


    def add_documents(self, documents):
        print("In add documents, class Vector Store")
        self.vectorstore = Chroma.from_documents(
            documents,
            self.embedding_model,
            persist_directory = self.persist_dir
        )


    def similarity_search(self, query, k: int = 25):
        print("In the vector store, similarity search.")
        if not self.vectorstore:
            raise ValueError("No documents indexed. Add documents first.")
        results = self.vectorstore.similarity_search(query, k=k)
        # Removed print statement that was showing metadata
        # for doc in results:
        #   print(doc.metadata, doc.page_content[:200])
        return results # Added return statement


    def load_local(self):
        # Loading also needs the embedding function
        self.vectorstore = Chroma(
            persist_directory = self.persist_dir,
            embedding_function = self.embedding_model.embed_query
        )

# doc_loader = DocumentLoader()
# text_chunker = TextChunker(500,50)
# embedding_model = EmbeddingModel(model_name='all-mpnet-base-v2')
# vector_store = VectorStore(embedding_model)
# vector_store.add_documents(text_chunker.chunk_documents(doc_loader.load_document(file_path)))

# vector_store.save_local('/content/vector_store')
# vector_store.similarity_search("Who is the speech addressed to?")
# vector_store.similarity_search("Main points of the speech?")

from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
import os
from google.colab import userdata

class DocumentSummarizer:
    def __init__(self, vector_store, model_name="llama-3.1-8b-instant"):
        self.model_name = model_name
        self.vector_store = vector_store
        # Ensure you have added GROQ_API to Colab Secrets
        # os.environ["GROQ_API_KEY"] = os.getenv("GroqAPIKey") # Removed the erroneous line
        os.environ["GROQ_API_KEY"] = userdata.get("GroqAPIKey") # Added this line to use Colab secrets
        self.chat_groq = ChatGroq(
            model=self.model_name,
            temperature=0.5
        )

    def summarize(self, query: str, top_k: int = 20): # Increased top_k to 20
        print("In summarize..")
        results = self.vector_store.similarity_search(query, k=top_k)
        # Removed metadata from the context string
        context = "".join([doc.page_content for doc in results])
        # print(context)
        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are an expert in summarizing documents."),
            ("user", "From the following text:\n{context}\n\n"
                     "Provide a meaningful and insightful summary related to the query" # Modified prompt
                     "DO NOT make up stuff. If the text is empty or does not contain information related to the query, say " # Modified prompt
                     "'Information not found in documents related to the query'.") # Modified prompt
        ])

        chain = prompt | self.chat_groq
        response = chain.invoke({"context": context, "query": query}) # Pass query to the chain
        # print("response: ", response)
        return response.content

    def make_notes(self, query: str = None, top_k: int = 25): # Increased top_k to 20
        if query:
            results = self.vector_store.similarity_search(query, k=top_k)
        else:
            # if no query, just take a representative sample
            results = self.vector_store.similarity_search("", k=top_k)

        # Removed metadata from the context string
        context = "".join([doc.page_content for doc in results])

        prompt = ChatPromptTemplate.from_messages([
            ("system",
             "You are an expert in analysing documents and creating meaningful notes."),
            ("user",
             "Based on the following text:\n{context}\n\n"
             "Create structured notes with:\n"
             "1. Key Points: [main ideas]\n"
             "2. Important Details: [supporting information]\n"
             "3. Actionable Insights: [what can be done]\n"
             "4. Additional Information: [any other relevant details from the text]")
        ])
        chain = prompt | self.chat_groq
        response = chain.invoke({"context": context})
        return response.content

    def create_flashcards(self, query:str):
        if not query:
              raise ValueError("Need to provide query!!")

        results = self.vector_store.similarity_search(query, k=25)
        # Removed metadata from the context string
        context = "".join([doc.page_content for doc in results])
        prompt = ChatPromptTemplate.from_messages([
            ("system",
             "You are an expert in creating flashcards based on the provided information."),
            ("user",
             "Based on the following text:\n{query}\n\n"
             "Create a flash card with:\n"
             "1. The flash card should contain a short (2 to 3 lines), memorable answer that captures the key point(s)]\n"
             "2. Keep the language simple and focused so it’s easy to memorize.\n"
             "3. If the concept is complex, break it down into a core definition, formula, or key takeaway.")
        ])

        chain = prompt | self.chat_groq
        response = chain.invoke({"query": query})
        return response.content

# ds = DocumentSummarizer(vector_store)
# print(ds.summarize("what are Recurrent Neural Networks?"))
# print(ds.make_notes("Recurrent Neural Networks"))
# print(ds.create_flashcards(query="""A
# Gaussian Process with a given kernel defines a prior over functions."""))
# query="""Now suppose that we wish to employ the strategy mentioned above, where we condition
# only on the 𝜏previous time steps, i.e., 𝑥𝑡−1,...,𝑥𝑡−𝜏, rather than the entire sequence history
# 𝑥𝑡−1,...,𝑥1. Whenever we can throw away the history beyond the previous 𝜏steps without
# any loss in predictive power, we say that the sequence satisfies a Markov condition, i.e., that
# the future is conditionally independent of the past, given the recent history. When 𝜏 = 1,
# we say that the data is characterized by a first-order Markov model, and when 𝜏 = 𝑘, we
# say that the data is characterized by a 𝑘th-order Markov model."""

# query="""The new policy has swept every restriction aside. Vessels of every kind, whatever their flag, their character, their cargo, their destination, their errand, have been ruthlessly sent to the bottom without warning and without thought of help or mercy for those on board, the vessels of friendly neutrals along with those of belligerents. Even hospital ships and ships carrying relief to the sorely bereaved and stricken people of Belgium, though the latter were provided with safe conduct through the proscribed areas by the German government itself and were distinguished by unmistakable marks of identity, have been sunk with the same reckless lack of compassion or of principle.

# I was for a little while unable to believe that such things would in fact be done by any government that had hitherto subscribed to the humane practices of civilized nations. International law had its origin in the attempt to set up some law which would be respected and observed upon the seas, where no nation had right of dominion and where lay the free highways of the world. By painful stage after stage has that law been built up, with meager enough results, indeed, after all was accomplished that could be accomplished, but always with a clear view, at least, of what the heart and conscience of mankind demanded."""

# print(ds.make_notes())

from typing import TypedDict, List, Optional, Annotated

class QuantumNotesState(TypedDict):
  file_path: str
  task:str
  query:Optional[str]
  documents: Optional[List[str]]
  summary: Optional[str]
  notes: Optional[str]
  flashcards: Optional[str]
  errors: Optional[List[Exception]]
  chunks: Optional[List[str]]
  embeddings: Optional[List[float]]
  vector_store: Optional[str]
  context: Optional[str]
  model_name: Optional[str]

class DocumentProcessingAgent:
  def __init__(self):
    self.doc_loader = DocumentLoader()
    self.text_chunker = TextChunker()
    self.embedding_model = EmbeddingModel()
    # self.vector_store = VectorStore()


  def load_documents_node(self, state:QuantumNotesState):
    print("In load_docs_node...")
    file_path = state.get('file_path', "")
    if file_path:
      print(f"Loading documents from {file_path}")
      documents = self.doc_loader.load_document(file_path)
      return {"documents": documents}
    else:
      return {"documents": []}

  def text_chunker_node(self, state:QuantumNotesState):
    print("In chunking")
    documents = state.get('documents', [])
    # chunk_size = state.get('chunk_size', 500)
    # chunk_overlap = state.get('chunk_overlap', 50)
    if documents:
      chunks =  self.text_chunker.chunk_documents(documents)
      return {"chunks": chunks}
    return {"chunks": []}

  def embeddings_and_vectorstore_node(self, state:QuantumNotesState):
    print("In embeddings and vectorstore")
    chunks = state.get("chunks", [])
    vector_store = VectorStore(self.embedding_model)
    if chunks:
      print("Adding documents to vector store...")
      # embeddings = self.embedding_model.embed_docs(chunks) # Embeddings are handled within VectorStore.add_documents
      print("After Embeddings..")
      vector_store.add_documents(chunks)
      return {"vector_store": vector_store} # No need to return embeddings separately

    return {"embeddings": [], "vector_store":[]}

class SummarizingAgent:
  def __init__(self, model_name:str="llama-3.1-8b-instant"):
    self.model_name = model_name
  def make_summary_node(self, state:QuantumNotesState):
    print("In make_summary")
    vector_store = state.get("vector_store")

    ds = DocumentSummarizer(vector_store, self.model_name)
    query = state.get("query", "")
    # query = state.get("summary_query", "")
    summary = ds.summarize(query)
    print("Printing summary....")
    return {"summary": summary}

class NoteMakingAgent:
  def __init__(self, model_name:str="llama-3.1-8b-instant"):
    self.model_name = model_name

  def make_notes_node(self, state:QuantumNotesState):
    print("In make_notes")
    vector_store = state.get("vector_store")
    ds = DocumentSummarizer(vector_store, self.model_name)
    query = state.get("query", "")
    # query = state.get("notes_query", "")
    notes = ds.make_notes(query)
    return {"notes": notes}

class FlashCardMakingAgent:
  def __init__(self, model_name:str="llama-3.1-8b-instant"):
    self.model_name = model_name

  def make_flashcards_node(self, state:QuantumNotesState):
    print("In make_flashcards")
    vector_store = state.get("vector_store")
    ds = DocumentSummarizer(vector_store, self.model_name)
    query = state.get("query", "")
    # query = state.get("flash_query", "")
    flashcard = ds.create_flashcards(query)
    return {"flashcards": flashcard}

from langgraph.graph import StateGraph, START, END

class QuantumNotesAgent:
  def __init__(self):
    self.dpa = DocumentProcessingAgent()
    self.sa = SummarizingAgent()
    self.nma = NoteMakingAgent()
    self.fca = FlashCardMakingAgent()
    self.graph = None

  def _check_task(self, state:QuantumNotesState):
    if state.get("task") == "summarize":
      return "make_summary"
    elif state.get("task") == "make_notes":
      return "make_notes"
    elif state.get("task") == "make_flashcards":
      return "make_flashcards"
    else:
      return END # or handle unsupported task

  def _build_graph(self):
    print(f"Builder...")
    builder = StateGraph(QuantumNotesState)
    builder.add_node("load_documents", self.dpa.load_documents_node)
    builder.add_node("text_chunker", self.dpa.text_chunker_node)
    builder.add_node("embeddings_and_vectorstore", self.dpa.embeddings_and_vectorstore_node)
    builder.add_node("make_summary", self.sa.make_summary_node)
    builder.add_node("make_notes", self.nma.make_notes_node)
    builder.add_node("make_flashcards", self.fca.make_flashcards_node)


    builder.add_edge(START, "load_documents")
    builder.add_edge("load_documents", "text_chunker")
    builder.add_edge("text_chunker", "embeddings_and_vectorstore")
    builder.add_conditional_edges("embeddings_and_vectorstore", self._check_task, ["make_summary", "make_notes", "make_flashcards"])
    builder.add_edge("make_summary", END)
    builder.add_edge("make_notes", END)
    builder.add_edge("make_flashcards", END)
    self.graph = builder.compile()

  def process_docs(self, state:QuantumNotesState):
    if not self.graph:
      self._build_graph()
    return self.graph.invoke(state)

initial_state = {
    "file_path": "/content/MCAKCA032-PRINCIPALES OF SOFT COMPUTING-SN SIVNANDAM AND DEEPA SN (1).pdf",
    "query": "what is artificial neural network? How does it differ from biological neural network?",
    "task": "make_notes"
}

qa = QuantumNotesAgent()
result = qa.process_docs(initial_state)
print(result['notes'])

initial_state = {
    "file_path": "/content/MCAKCA032-PRINCIPALES OF SOFT COMPUTING-SN SIVNANDAM AND DEEPA SN (1).pdf",
    "query": "Explain the concept of Hebb Network",
    "task": "make_notes"
}

qa = QuantumNotesAgent()
result = qa.process_docs(initial_state)
print(result['notes'])

initial_state = {
    "file_path": "/content/MCAKCA032-PRINCIPALES OF SOFT COMPUTING-SN SIVNANDAM AND DEEPA SN (1).pdf",
    "query": "Explain what is Backpropagation network and what are the learning factors of Backpropagation network?",
    "task": "make_notes"
}

qa = QuantumNotesAgent()
result = qa.process_docs(initial_state)
print(result['notes'])

initial_state = {
    "file_path": "/content/MCAKCA032-PRINCIPALES OF SOFT COMPUTING-SN SIVNANDAM AND DEEPA SN (1).pdf",
    "query": "What is Linear vector quantization? Generate 5 flashcards",
    "task": "make_flashcards"
}
qa = QuantumNotesAgent()
result = qa.process_docs(initial_state)
print(result["flashcards"])

initial_state = {
    "file_path": "/content/MCAKCA032-PRINCIPALES OF SOFT COMPUTING-SN SIVNANDAM AND DEEPA SN (1).pdf",
    "query": "What is the concept of Learning Vector Quantization?",
    "task": "make_notes"
}

qa = QuantumNotesAgent()
result = qa.process_docs(initial_state)
print(result['notes'])

initial_state = {
    "file_path": "/content/lecture_transcript.txt",
    "query": "Summarize the key concepts covered.",
    "task": "summarize"
}
qa = QuantumNotesAgent()
result = qa.process_docs(initial_state)
print(result["summary"])

initial_state = {
    "file_path": "/content/ana-bell-v1.parquet",
    "query": "Summarize the key concepts covered.",
    "task": "summarize"
}
qa = QuantumNotesAgent()
result = qa.process_docs(initial_state)
print(result["summary"])

initial_state = {
    "file_path": "/content/ana-bell-v1.parquet",
    "query": "What is programming?",
    "task": "make_flashcards"
}
qa = QuantumNotesAgent()
result = qa.process_docs(initial_state)
print(result["flashcards"])

initial_state = {
    "file_path": "/content/speech.txt",
    "query": "The text emphasizes the importance of fighting for fundamental rights and freedoms, including democracy, individual voice in governance, and the rights of small nations. It also highlights the goal of achieving a universal dominion of right through international cooperation among free peoples, with the ultimate aim of bringing peace, safety, and freedom to all nations.",
    "task": "make_notes"
}

qa = QuantumNotesAgent()
result = qa.process_docs(initial_state)

print(result.keys())

print(result['notes'])

result['task']

result['query']

result['summary']

initial_state = {
    "file_path": "/content/speech.txt",
    "task": "make_notes"
}

qa = QuantumNotesAgent()
result = qa.process_docs(initial_state)
print(result['notes'])

initial_state = {
    "file_path": "/content/speech.txt",
    "query": "Sovereignity means supreme power especially over a body politic.",
    "task": "make_flashcards"
}

qa = QuantumNotesAgent()
result = qa.process_docs(initial_state)
print(result['notes'])

print(result.keys())

print(result['flashcards'])

