# -*- coding: utf-8 -*-
"""Final Transformer RAG Pipeline SmartLearn_AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h7NBMpDjXb6HDsPDcykZakhpGSY4A2eh

# **SETUP**
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -r requirements.txt

!pip install -U langchain-community

# Core dependencies
!pip install transformers torch accelerate bitsandbytes
!pip install sentence-transformers langchain chromadb
!pip install pandas numpy

# Optional for better performance
!pip install flash-attn --no-build-isolation

# For HuggingFace authentication (if needed)
!pip install huggingface_hub

# Save package requirements to Google Drive
!pip freeze > /content/drive/MyDrive/requirements.txt
print("Saved requirements.txt to Google Drive.")

!pip install faster-whisper faiss-cpu peft

!pip install ffmpeg-python nltk

import os, json, math, re, glob, uuid, shutil, gc, pathlib, datetime, pickle
import numpy as np
import pandas as pd
from tqdm import tqdm
import ffmpeg
import nltk
nltk.download('punkt')

BASE = "/content"
DATA = f"{BASE}/data"
os.makedirs(f"{DATA}/videos", exist_ok=True)
os.makedirs(f"{DATA}/audio", exist_ok=True)
os.makedirs(f"{DATA}/transcripts", exist_ok=True)
os.makedirs(f"{DATA}/transcripts_raw", exist_ok=True)
os.makedirs(f"{DATA}/chunks", exist_ok=True)
os.makedirs(f"{DATA}/index", exist_ok=True)

"""# **Whisper with Medium**"""

from faster_whisper import WhisperModel
import gc

# Full accuracy model
WHISPER_MODEL = "medium"
# Initialize the model globally
model = WhisperModel(WHISPER_MODEL, device="cuda", compute_type="float16")
gc.collect()


def transcribe_audio_to_parquet(audio_path, video_id):
    # Use the globally initialized model
    segments, info = model.transcribe(
        audio_path,
        beam_size=5,       # higher accuracy, a bit slower
        vad_filter=True,   # cuts silence
        word_timestamps=False
    )
    rows = []
    for i, seg in enumerate(segments):
        rows.append({
            "video_id": video_id,
            "segment_idx": i,
            "start_ts": float(seg.start),
            "end_ts": float(seg.end),
            "text": seg.text.strip()
        })
    out_path = f"{DATA}/transcripts/{video_id}.parquet"
    pd.DataFrame(rows).to_parquet(out_path, index=False)
    print(f"[{video_id}] Transcript saved:", out_path)
    return out_path

"""# **Unified upload & routing (video / audio / transcript)**"""

!pip install --index-url https://pypi.org/simple/ webvtt-py pysrt
from google.colab import files
import webvtt, pysrt
import ffmpeg

VIDEO_EXT = {".mp4", ".mkv", ".mov", ".avi"}
AUDIO_EXT = {".wav", ".mp3", ".m4a", ".flac", ".aac", ".ogg"}
TRANSCRIPT_EXT = {".txt", ".srt", ".vtt", ".json"}

def base_stem(fname): return pathlib.Path(fname).stem

def save_df_as_parquet(rows, video_id):
    df = pd.DataFrame(rows)
    df.insert(0, "video_id", video_id)
    out_path = f"{DATA}/transcripts/{video_id}.parquet"
    df.to_parquet(out_path, index=False)
    print("Transcript saved:", out_path)
    return out_path

def get_audio_duration(audio_path):
    try:
        probe = ffmpeg.probe(audio_path)
        return float(probe['format']['duration'])
    except ffmpeg.Error as e:
        print(f"Error getting duration for {audio_path}: {e.stderr.decode()}")
        return 0.0

def audio_from_video(video_path, out_audio_path, sr=16000):
    if not os.path.exists(out_audio_path):
        (
            ffmpeg
            .input(video_path)
            .output(out_audio_path, ac=1, ar=sr, format='wav', loglevel="error")
            .overwrite_output()
            .run()
        )
        print("Extracted audio:", out_audio_path)
    return out_audio_path

# transcript parsers
def parse_txt(path):
    with open(path, 'r', encoding='utf-8', errors='ignore') as f:
        text = f.read().strip()
    return [{"segment_idx": 0, "start_ts": 0.0, "end_ts": 0.0, "text": text}]

def parse_srt(path):
    subs = pysrt.open(path, encoding='utf-8')
    return [{"segment_idx": i, "start_ts": s.start.ordinal/1000.0, "end_ts": s.end.ordinal/1000.0, "text": s.text.replace('\n',' ').strip()} for i, s in enumerate(subs)]

def parse_vtt(path):
    rows=[]
    def hms_to_s(hms):
        h, m, s = hms.split(':')
        return int(h)*3600 + int(m)*60 + float(s)
    for i, cap in enumerate(webvtt.read(path)):
        rows.append({"segment_idx": i, "start_ts": hms_to_s(cap.start), "end_ts": hms_to_s(cap.end), "text": cap.text.replace('\n',' ').strip()})
    return rows

def parse_whisper_json(path):
    with open(path, 'r', encoding='utf-8', errors='ignore') as f:
        obj = json.load(f)
    segs = obj.get("segments", [])
    return [{"segment_idx": i, "start_ts": float(s.get("start", 0.0)), "end_ts": float(s.get("end", 0.0)), "text": str(s.get("text","")).strip()} for i, s in enumerate(segs)]

def route_and_process_uploaded(files_dict):
    for fname in files_dict.keys():
        local = os.path.join("/content", fname)
        ext = pathlib.Path(fname).suffix.lower()
        vid = base_stem(fname)

        if ext in VIDEO_EXT:
            dst = f"{DATA}/videos/{fname}"
            shutil.move(local, dst)
            print("Stored video:", dst)
            audio_path = f"{DATA}/audio/{vid}.wav"
            audio_from_video(dst, audio_path)

            duration = get_audio_duration(audio_path)
            if duration > 120 * 60: # 120 minutes (2 hours) in seconds
                print(f"Warning: Audio duration ({duration:.1f}s) exceeds 2 hours. Processing may take a long time.")
                print("Consider using a smaller Whisper model or be patient.")

            transcribe_audio_to_parquet(audio_path, vid)

        elif ext in AUDIO_EXT:
            dst = f"{DATA}/audio/{vid}.wav"
            if ext == ".wav":
                shutil.move(local, dst)
            else:
                (
                    ffmpeg
                    .input(local)
                    .output(dst, ac=1, ar=16000, format='wav', loglevel="error")
                    .overwrite_output()
                    .run()
                )
            print("Stored audio:", dst)

            duration = get_audio_duration(dst)
            if duration > 120 * 60: # 120 minutes (2 hours) in seconds
                print(f"Warning: Audio duration ({duration:.1f}s) exceeds 2 hours. Processing may take a long time.")
                print("Consider using a smaller Whisper model or be patient.")

            transcribe_audio_to_parquet(dst, vid)

        elif ext in TRANSCRIPT_EXT:
            raw_dst = f"{DATA}/transcripts_raw/{fname}"
            shutil.move(local, raw_dst)
            print("Stored raw transcript:", raw_dst)
            if ext == ".txt": rows = parse_txt(raw_dst)
            elif ext == ".srt": rows = parse_srt(raw_dst)
            elif ext == ".vtt": rows = parse_vtt(raw_dst)
            else: rows = parse_whisper_json(raw_dst)
            save_df_as_parquet(rows, base_stem(fname))

        else:
            print(f"Skipped unsupported file: {fname}")

print("Upload *any mix* of video/audio/transcript files…")
up = files.upload()
route_and_process_uploaded(up)

"""# **Text cleanup helpers (light normalization)**"""

import re
import pandas as pd
import glob, os

def normalize_whitespace(t: str) -> str:
    t = re.sub(r'\s+', ' ', t)
    return t.strip()

def basic_cleanup(t: str) -> str:
    # Minimal + safe: keep it simple to avoid altering technical terms
    t = t.replace("’","'").replace("“","\"").replace("”","\"").replace("–","-")
    t = normalize_whitespace(t)
    return t

"""# **Sentence segmentation with proportional timestamps**"""

import nltk
from nltk.tokenize import sent_tokenize
nltk.download('punkt')

def sentence_time_expand(seg_text, seg_start, seg_end):
    txt = basic_cleanup(seg_text)
    sents = [s for s in sent_tokenize(txt) if s.strip()]
    if not sents:
        return []
    total_chars = sum(len(s) for s in sents)
    if total_chars == 0:
        return []
    dur = max(0.0, float(seg_end) - float(seg_start))
    out=[]
    cur = float(seg_start)
    for s in sents:
        frac = len(s) / total_chars
        sdur = frac * dur
        out.append({"text": s, "start_ts": cur, "end_ts": cur + sdur})
        cur += sdur
    # snap last one exactly to seg_end (avoid floating drift)
    out[-1]["end_ts"] = float(seg_end)
    return out

def build_sentence_table(transcript_parquet_path: str) -> pd.DataFrame:
    df = pd.read_parquet(transcript_parquet_path)
    rows=[]
    for _, r in df.iterrows():
        exp = sentence_time_expand(r["text"], r["start_ts"], r["end_ts"])
        rows.extend(exp)
    if not rows:  # fallback: one sentence with no timing
        rows = [{"text": basic_cleanup(" ".join(df["text"].tolist())), "start_ts": 0.0, "end_ts": 0.0}]
    sents_df = pd.DataFrame(rows)
    return sents_df

"""# **Chunk builder (overlapping, sentence-aware)**
Creates overlapping chunks that respect sentence boundaries.
Uses character-based size (simple & robust).
Stores approx. start/end timestamps per chunk from contained sentences.
"""

def make_chunks_from_sentences(sents_df: pd.DataFrame, max_chars=1800, overlap_chars=200):
    """
    Build sentence-aware overlapping chunks.
    Returns a list of dicts: {text, start_ts, end_ts}
    """
    chunks=[]
    buf_text=""
    buf_starts=[]
    buf_ends=[]

    def flush_buffer():
        if not buf_text.strip():
            return
        chunks.append({
            "text": buf_text.strip(),
            "start_ts": min(buf_starts) if buf_starts else 0.0,
            "end_ts": max(buf_ends) if buf_ends else 0.0
        })

    for _, row in sents_df.iterrows():
        s = str(row["text"]).strip()
        st, et = float(row["start_ts"]), float(row["end_ts"])
        if not s:
            continue

        # If sentence fits, append
        if len(buf_text) + len(s) + 1 <= max_chars:
            buf_text = (buf_text + " " + s).strip() if buf_text else s
            buf_starts.append(st)
            buf_ends.append(et)
        else:
            # flush current buffer
            flush_buffer()
            # start next buffer with overlap
            if overlap_chars > 0 and len(buf_text) > 0:
                # carry last overlap_chars worth of text, but we need to recompute times.
                # For simplicity: start a fresh buffer from current sentence.
                # (If you want *strict* text overlap, you can keep trailing text, but times
                # then should span both old+new which is less precise.)
                pass
            # new buffer with current sentence
            buf_text = s
            buf_starts = [st]
            buf_ends = [et]

    # final flush
    flush_buffer()
    return chunks

"""#**Run sentence → chunks for all transcripts and save**
This will produce chunk files at /content/data/chunks/<video_id>_chunks.parquet with the schema:
video_id, chunk_idx, start_ts, end_ts, text
"""

import nltk
nltk.download("punkt")
nltk.download("punkt_tab")

BASE = "/content"
DATA = f"{BASE}/data"
TRANSCRIPTS_GLOB = f"{DATA}/transcripts/*.parquet"

def build_and_save_chunks(transcript_path: str, max_chars=1800, overlap_chars=200):
    video_id = os.path.splitext(os.path.basename(transcript_path))[0]
    sents_df = build_sentence_table(transcript_path)
    chunks = make_chunks_from_sentences(sents_df, max_chars=max_chars, overlap_chars=overlap_chars)

    out_rows=[]
    for i, c in enumerate(chunks):
        out_rows.append({
            "video_id": video_id,
            "chunk_idx": i,
            "start_ts": float(c["start_ts"]),
            "end_ts": float(c["end_ts"]),
            "text": c["text"]
        })
    cdf = pd.DataFrame(out_rows)
    outp = f"{DATA}/chunks/{video_id}_chunks.parquet"
    cdf.to_parquet(outp, index=False)
    print(f"Chunks saved: {outp}  |  chunks={len(cdf)}  avg_chars≈{int(cdf['text'].map(len).mean()) if len(cdf) else 0}")
    return outp

# Build chunks for every transcript available
for tp in glob.glob(TRANSCRIPTS_GLOB):
    build_and_save_chunks(tp, max_chars=1800, overlap_chars=200)

"""# **Sanity checks (quick QA before Vector DB)**"""

import pandas as pd, textwrap as tw, random, glob, os

def peek_chunks(video_id=None, n=3):
    paths = glob.glob(f"{DATA}/chunks/*_chunks.parquet")
    if not paths:
        raise RuntimeError("No chunk files found. Did Step 6 run?")
    if video_id:
        paths = [p for p in paths if os.path.basename(p).startswith(video_id)]
        if not paths:
            raise RuntimeError(f"No chunks for video_id={video_id}")
    path = paths[0]
    df = pd.read_parquet(path)
    print("File:", os.path.basename(path), "total chunks:", len(df))
    for _, r in df.sample(min(n, len(df)), random_state=42).iterrows():
        print(f"\n[# {int(r['chunk_idx'])}] {r['start_ts']:.1f}–{r['end_ts']:.1f}s")
        print("-" * 80)
        print(tw.fill(r["text"], width=100))

peek_chunks()  # or peek_chunks("video_smartlearnai")

"""### **Tiny heuristic to check coverage and typical chunk length.**"""

import numpy as np

def chunk_stats(video_id=None):
    paths = glob.glob(f"{DATA}/chunks/*_chunks.parquet")
    if video_id:
        paths = [p for p in paths if os.path.basename(p).startswith(video_id)]
    if not paths:
        print("No chunks found.")
        return
    for p in paths:
        df = pd.read_parquet(p)
        lens = df["text"].map(len).to_numpy()
        print(os.path.basename(p),
              "| chunks:", len(df),
              "| chars mean/med:", int(lens.mean()), "/", int(np.median(lens)),
              "| min/max:", lens.min(), "/", lens.max())

chunk_stats()

import pandas as pd

# Load your parquet file
file_path = "/content/data/chunks/ana-bell-v1_chunks.parquet"   # change to your actual filename
df = pd.read_parquet(file_path)

# Show first 5 rows
print("Preview of Parquet file:")
print(df.head())

# Show column names
print("\n Columns in this file:")
print(df.columns.tolist())

from huggingface_hub import login

# Replace 'your_token_here' with your actual HuggingFace token
login(token="your_token_ID")

# ========================================
# RAG with vLLM + Chroma + Mistral 7B
# ========================================
import os, glob, json
import pandas as pd
from langchain.docstore.document import Document


# ========================================
#  Step 1: Load transcript chunks
# ========================================
def load_chunks(file_path, column_name="text"):
    """Load transcript chunks from parquet, txt, or json."""
    if file_path.endswith(".parquet"):
        df = pd.read_parquet(file_path)
        if column_name not in df.columns:
            raise ValueError(f"Column '{column_name}' not found. Available: {df.columns.tolist()}")
        chunks = df[column_name].dropna().tolist()

    elif file_path.endswith(".txt"):
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read().split("\n")
        chunks = [line.strip() for line in text if line.strip()]

    elif file_path.endswith(".json"):
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if column_name not in data:
            raise ValueError(f"Key '{column_name}' not found in JSON. Available: {list(data.keys())}")
        chunks = data[column_name]

    else:
        raise ValueError("Supported formats: .parquet, .txt, .json")

    return chunks


CHUNKS_DIR = "/content/data/chunks"
chunk_files = glob.glob(os.path.join(CHUNKS_DIR, "*_chunks.parquet"))
print(f"Found {len(chunk_files)} chunk files in {CHUNKS_DIR}")

# ========================================
#  Step 2: Embeddings with SentenceTransformer
# ========================================
from langchain_core.embeddings import Embeddings
from langchain.vectorstores import Chroma
from sentence_transformers import SentenceTransformer

import gc
class SentenceTransformerEmbeddings(Embeddings):
    def __init__(self, model_name="sentence-transformers/all-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)

    def embed_documents(self, texts):
        return self.model.encode(texts).tolist()

    def embed_query(self, text):
        return self.model.encode(text).tolist()


embedder = SentenceTransformerEmbeddings("sentence-transformers/all-MiniLM-L6-v2")

all_docs = []
for file_path in chunk_files[:5]:
    try:
        chunks = load_chunks(file_path, column_name="text")
        docs = [Document(page_content=chunk) for chunk in chunks]
        all_docs.extend(docs)
        print(f"Processed: {file_path}")
    except Exception as e:
        print(f"Error processing {file_path}: {e}")
    gc.collect()

# ========================================
#  Step 3: Create vector store (Chroma)
# ========================================
# Create and persist ChromaDB on Google Drive
vectorstore = Chroma.from_documents(
    all_docs,
    embedder,
    persist_directory="/content/drive/MyDrive/chroma_db"  # Persistent storage
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
print("Chroma vector store created and saved to Google Drive")

#Loading the vector embeddings and vectorDB
from google.colab import drive
drive.mount('/content/drive')

# Just load the existing ChromaDB - no embedding generation needed!
vectorstore = Chroma(
    persist_directory="/content/drive/MyDrive/chroma_db",
    embedding_function=embedder  # Same embedding function as before
)

retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
print("Loaded existing ChromaDB from Drive!")

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

transformer_model = None

def main():
    global transformer_model
    model_name = "mistralai/Mistral-7B-Instruct-v0.3"

    # Load tokenizer and model separately
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        max_position_embeddings=2048  # Equivalent to max_model_len
    )

    # Create pipeline with loaded model and tokenizer
    transformer_model = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        temperature=0.0,
        do_sample=False,
        return_full_text=False,
        pad_token_id=tokenizer.eos_token_id
    )

# ========================================
#  Step 5: RAG Query Function
# ========================================
def answer_query(query):
    docs = retriever.get_relevant_documents(query)
    context = " ".join([d.page_content for d in docs])

    prompt = f"""[INST] Use the following context to answer the question clearly.

    Context: {context}

    Question: {query}
    [/INST]"""

    outputs = transformer_model(
        prompt,
        max_new_tokens=512,
        temperature=0.0,
        do_sample=False,
        return_full_text=False
    )
    return outputs[0]["generated_text"].strip()

if __name__ == '__main__':
    main()

# ========================================
#  Step 6: Test Query
# ========================================

query = "Summarize the main idea of this podcast."
print("\nQuery:", query)
print("Answer:", answer_query(query))

# ========================================
#  Step 6: Test Query
# ========================================

query = "Generate a point wise highlights discussed in the podcast"
print("\nQuery:", query)
print("Answer:", answer_query(query))