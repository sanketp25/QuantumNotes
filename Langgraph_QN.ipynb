{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeiYb4b3eKT3",
        "outputId": "5de9a704-e252-483c-fad4-416ff861ce96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cudf-cu12@ https://pypi.nvidia.com/cudf-cu12/cudf_cu12-25.6.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (from -r requirements.txt (line 73))\n",
            "  Downloading https://pypi.nvidia.com/cudf-cu12/cudf_cu12-25.6.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting en_core_web_sm@ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl#sha256=1932429db727d4bff3deed6b34cfc05df17794f4a52eeb26cf8928f7c1a0fb85 (from -r requirements.txt (line 113))\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hProcessing /colabtools/dist/google_colab-1.0.0.tar.gz (from -r requirements.txt (line 183))\n",
            "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/colabtools/dist/google_colab-1.0.0.tar.gz'\n",
            "\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3Ht5DHIrtYS",
        "outputId": "e181d70e-f4f6-496c-ada2-fd1193639de7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.29)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (0.6.7)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.1.0)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.76)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.28)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.1.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.6.4)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
            "Requirement already satisfied: faiss-gpu-cu12 in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.12/dist-packages (from faiss-gpu-cu12) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-gpu-cu12) (25.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.12/dist-packages (from faiss-gpu-cu12) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from faiss-gpu-cu12) (12.6.4.1)\n",
            "Collecting langchain_ollama\n",
            "  Using cached langchain_ollama-0.3.8-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting ollama<1.0.0,>=0.5.3 (from langchain_ollama)\n",
            "  Using cached ollama-0.5.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.76 in /usr/local/lib/python3.12/dist-packages (from langchain_ollama) (0.3.76)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_ollama) (0.4.28)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_ollama) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_ollama) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_ollama) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_ollama) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_ollama) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_ollama) (2.11.9)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama<1.0.0,>=0.5.3->langchain_ollama) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain_ollama) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain_ollama) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain_ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain_ollama) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain_ollama) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain_ollama) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_ollama) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_ollama) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_ollama) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_ollama) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_ollama) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_ollama) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_ollama) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_ollama) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain_ollama) (1.3.1)\n",
            "Using cached langchain_ollama-0.3.8-py3-none-any.whl (25 kB)\n",
            "Using cached ollama-0.5.4-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: ollama, langchain_ollama\n",
            "Successfully installed langchain_ollama-0.3.8 ollama-0.5.4\n",
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.3.8-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (0.3.76)\n",
            "Collecting groq<1,>=0.30.0 (from langchain-groq)\n",
            "  Downloading groq-0.31.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (2.11.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (4.15.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (0.4.28)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (25.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.30.0->langchain-groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.75->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain-groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain-groq) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (2.5.0)\n",
            "Downloading langchain_groq-0.3.8-py3-none-any.whl (16 kB)\n",
            "Downloading groq-0.31.1-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain-groq\n",
            "Successfully installed groq-0.31.1 langchain-groq-0.3.8\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community langgraph transformers torch sentence-transformers pypdf pymupdf python-docx\n",
        "!pip install faiss-gpu-cu12\n",
        "!pip install langchain_ollama\n",
        "!pip install langchain-groq\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "from google.colab import userdata\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "from langchain_community.document_loaders import PyMuPDFLoader, TextLoader\n",
        "from typing import List\n",
        "\n",
        "class DocumentLoader:\n",
        "  def __init__(self):\n",
        "    self.supported_extensions = {'.pdf', '.txt'}\n",
        "\n",
        "\n",
        "  def _valid_file(self, loaded_file):\n",
        "    if loaded_file in self.supported_extensions:\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def _load_pdf(self, file_path):\n",
        "    loader = PyMuPDFLoader(file_path)\n",
        "    return loader.load()\n",
        "\n",
        "  def _load_text(self, file_path):\n",
        "    loader = TextLoader(file_path)\n",
        "    return loader.load()\n",
        "\n",
        "\n",
        "  def load_document(self, file_path):\n",
        "    \"\"\"\n",
        "    Loads documents and stores them as lists\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    # print(f\"Loading documents from DL class\")\n",
        "    if self._valid_file(Path(file_path).suffix):\n",
        "      if Path(file_path).suffix == '.pdf':\n",
        "        documents.extend(self._load_pdf(file_path))\n",
        "      else: documents.extend(self._load_text(file_path))\n",
        "    # print(f\"Loaded Documents: {documents}\")\n",
        "    return documents\n"
      ],
      "metadata": {
        "id": "Wr5IaYuc6wws"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "class TextChunker:\n",
        "  def __init__(self, chunk_size=100, chunk_overlap=0):\n",
        "    self.chunk_size = chunk_size\n",
        "    self.chunk_overlap = chunk_overlap\n",
        "\n",
        "    self.recursive_splitter =  RecursiveCharacterTextSplitter(chunk_size = self.chunk_size, chunk_overlap= self.chunk_overlap, separators=[\"\\n\\n\", \"\\n\", \".\"])\n",
        "\n",
        "  def chunk_documents(self, documents, strategy = 'rec'):\n",
        "    if strategy=='rec':\n",
        "      return self.recursive_splitter.split_documents(documents)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wri7J6Vu9zoe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file_path = '/content/DeepLearningBook.pdf'\n",
        "file_path = '/content/speech.txt'"
      ],
      "metadata": {
        "id": "60dbGkDdwZst"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_loader = DocumentLoader()\n",
        "text_chunker = TextChunker(500,50)\n",
        "print(text_chunker.chunk_documents(doc_loader.load_document(file_path))[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZcf6gAnuaxU",
        "outputId": "e13b0d59-7c5d-4c15-f072-770117cbd2ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='I have called the Congress into extraordinary session because there are serious, very serious, choices of policy to be made, and made immediately, which it was neither right nor constitutionally permissible that I should assume the responsibility of making' metadata={'source': '/content/speech.txt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "class EmbeddingModel:\n",
        "  def __init__(self, model_name = 'all-mpnet-base-v2'):\n",
        "    self.model_name = model_name\n",
        "    self.model = SentenceTransformer(self.model_name)\n",
        "\n",
        "  def embed_docs(self, documents):\n",
        "    print(f\"Embedding documents...\")\n",
        "    texts = [doc.page_content for doc in documents]\n",
        "    embeddings = self.model.encode(texts, convert_to_tensor=False).tolist()\n",
        "    return embeddings\n",
        "\n",
        "  def embed_query(self, query):\n",
        "    print(f\"In embed query with query: {query}\")\n",
        "    return self.model.encode(query,convert_to_tensor=False).tolist()\n",
        "\n",
        "  def get_model(self):\n",
        "    return self.model\n",
        "\n",
        "\n",
        "\n",
        "# model = SentenceTransformer(\"\")\n",
        "# embeddings ="
      ],
      "metadata": {
        "id": "RruB-mIF2jh6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain.schema import Document\n",
        "import faiss\n",
        "\n",
        "class VectorStore:\n",
        "    def __init__(self, embedding_model):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.faiss_index = None\n",
        "        self.docstore = InMemoryDocstore()\n",
        "        self.vectorstore = None\n",
        "        self.documents = []\n",
        "\n",
        "    def _embedding_function(self, x):\n",
        "        if isinstance(x, str):\n",
        "          return self.embedding_model.embed_query(x)\n",
        "        return self.embedding_model.embed_docs\n",
        "\n",
        "\n",
        "    def add_documents(self, embeddings, documents):\n",
        "        print(\"In add documnets, class Vector Store\")\n",
        "        embeddings = np.array(embeddings).astype('float32')\n",
        "        dim = embeddings.shape[1]\n",
        "        # Create or append FAISS index\n",
        "        if self.faiss_index is None:\n",
        "            self.faiss_index = faiss.IndexFlatL2(dim)\n",
        "        self.faiss_index.add(embeddings)\n",
        "        self.documents.extend(documents)\n",
        "        for i, doc in enumerate(documents):\n",
        "            # print(\"In enumerate docs\")\n",
        "            # Corrected line: Passing a dictionary {doc_id: doc}\n",
        "            self.docstore.add({str(len(self.documents)-len(documents)+i): doc})\n",
        "        # Wrap in LangChain's FAISS VectorStore\n",
        "        self.vectorstore = FAISS(\n",
        "            embedding_function=self._embedding_function,  # expects a string, returns a list[float]\n",
        "            index=self.faiss_index,\n",
        "            docstore=self.docstore,\n",
        "            index_to_docstore_id={i: str(i) for i in range(len(self.documents))}\n",
        "        )\n",
        "        print(self.vectorstore)\n",
        "\n",
        "\n",
        "    def similarity_search(self, query: str, k: int = 5):\n",
        "        print(\"In the vector store, similarity search.\")\n",
        "        if not self.vectorstore:\n",
        "            raise ValueError(\"No documents indexed. Add documents first.\")\n",
        "        return self.vectorstore.similarity_search(query, k=k)\n",
        "\n",
        "    def save_local(self, file_path: str):\n",
        "        if not self.vectorstore:\n",
        "            raise ValueError(\"No vectorstore to save.\")\n",
        "        self.vectorstore.save_local(file_path)\n",
        "\n",
        "    def load_local(self, file_path: str):\n",
        "        # Loading also needs the embedding function\n",
        "        self.vectorstore = FAISS.load_local(\n",
        "            file_path,\n",
        "            self.embedding_model.embed_docs,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )"
      ],
      "metadata": {
        "id": "N6bmpr0wq9jX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# doc_loader = DocumentLoader()\n",
        "# text_chunker = TextChunker(500,50)\n",
        "# embedding_model = EmbeddingModel(model_name='all-mpnet-base-v2')\n",
        "# vector_store = VectorStore(embedding_model)\n",
        "# vector_store.add_documents(text_chunker.chunk_documents(doc_loader.load_document(file_path)))\n",
        "\n",
        "# vector_store.save_local('/content/vector_store')\n",
        "# vector_store.similarity_search(\"Who is the speech addressed to?\")\n",
        "# vector_store.similarity_search(\"Main points of the speech?\")"
      ],
      "metadata": {
        "id": "uYiYLfXWP-hp"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "class DocumentSummarizer:\n",
        "    def __init__(self, vector_store, model_name=\"llama-3.1-8b-instant\"):\n",
        "        self.model_name = model_name\n",
        "        self.vector_store = vector_store\n",
        "        os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API\")\n",
        "        self.chat_groq = ChatGroq(\n",
        "            model=self.model_name,\n",
        "            temperature=0.5\n",
        "        )\n",
        "\n",
        "    def summarize(self, query: str, top_k: int = 5):\n",
        "        print(\"In summarize..\")\n",
        "        results = self.vector_store.similarity_search(query, k=top_k)\n",
        "        context = \"\".join([doc for doc in results[0].page_content])\n",
        "        # print(context)\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"You are an expert in summarizing documents.\"),\n",
        "            (\"user\", \"From the following text:\\n{context}\\n\\n\"\n",
        "                     \"Provide a meaningful and insightful summary. \"\n",
        "                     \"DO NOT make up stuff. If the text is empty, say \"\n",
        "                     \"'Information not found in documents'.\")\n",
        "        ])\n",
        "\n",
        "        chain = prompt | self.chat_groq\n",
        "        response = chain.invoke({\"context\": context})\n",
        "        # print(\"response: \", response)\n",
        "        return response.content\n",
        "\n",
        "    def make_notes(self, query: str = None, top_k: int = 10):\n",
        "        if query:\n",
        "            results = self.vector_store.similarity_search(query, k=top_k)\n",
        "        else:\n",
        "            # if no query, just take a representative sample\n",
        "            results = self.vector_store.similarity_search(\"\", k=top_k)\n",
        "\n",
        "        context = \"\".join([doc for doc in results[0].page_content])\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\",\n",
        "             \"You are an expert in analysing documents and creating meaningful notes.\"),\n",
        "            (\"user\",\n",
        "             \"Based on the following text:\\n{context}\\n\\n\"\n",
        "             \"Create structured notes with:\\n\"\n",
        "             \"1. Key Points: [main ideas]\\n\"\n",
        "             \"2. Important Details: [supporting information]\\n\"\n",
        "             \"3. Actionable Insights: [what can be done]\")\n",
        "        ])\n",
        "        chain = prompt | self.chat_groq\n",
        "        response = chain.invoke({\"context\": context})\n",
        "        return response.content\n",
        "\n",
        "    def create_flashcards(self, query:str):\n",
        "        if not query:\n",
        "              raise ValueError(\"Need to provide query!!\")\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\",\n",
        "             \"You are an expert in creating flashcards based on the provided information.\"),\n",
        "            (\"user\",\n",
        "             \"Based on the following text:\\n{query}\\n\\n\"\n",
        "             \"Create a flash card with:\\n\"\n",
        "             \"1. The flash card should contain a short (2 to 3 lines), memorable answer that captures the key point(s)]\\n\"\n",
        "             \"2. Keep the language simple and focused so it‚Äôs easy to memorize.\\n\"\n",
        "             \"3. If the concept is complex, break it down into a core definition, formula, or key takeaway.\")\n",
        "        ])\n",
        "\n",
        "        chain = prompt | self.chat_groq\n",
        "        response = chain.invoke({\"query\": query})\n",
        "        return response.content\n",
        "\n"
      ],
      "metadata": {
        "id": "1zOysNKe94xu"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ds = DocumentSummarizer(vector_store)\n",
        "# print(ds.summarize(\"what are Recurrent Neural Networks?\"))\n",
        "# print(ds.make_notes(\"Recurrent Neural Networks\"))\n",
        "# print(ds.create_flashcards(query=\"\"\"A\n",
        "# Gaussian Process with a given kernel defines a prior over functions.\"\"\"))\n",
        "# query=\"\"\"Now suppose that we wish to employ the strategy mentioned above, where we condition\n",
        "# only on the ùúèprevious time steps, i.e., ùë•ùë°‚àí1,...,ùë•ùë°‚àíùúè, rather than the entire sequence history\n",
        "# ùë•ùë°‚àí1,...,ùë•1. Whenever we can throw away the history beyond the previous ùúèsteps without\n",
        "# any loss in predictive power, we say that the sequence satisfies a Markov condition, i.e., that\n",
        "# the future is conditionally independent of the past, given the recent history. When ùúè = 1,\n",
        "# we say that the data is characterized by a first-order Markov model, and when ùúè = ùëò, we\n",
        "# say that the data is characterized by a ùëòth-order Markov model.\"\"\"\n",
        "\n",
        "# query=\"\"\"The new policy has swept every restriction aside. Vessels of every kind, whatever their flag, their character, their cargo, their destination, their errand, have been ruthlessly sent to the bottom without warning and without thought of help or mercy for those on board, the vessels of friendly neutrals along with those of belligerents. Even hospital ships and ships carrying relief to the sorely bereaved and stricken people of Belgium, though the latter were provided with safe conduct through the proscribed areas by the German government itself and were distinguished by unmistakable marks of identity, have been sunk with the same reckless lack of compassion or of principle.\n",
        "\n",
        "# I was for a little while unable to believe that such things would in fact be done by any government that had hitherto subscribed to the humane practices of civilized nations. International law had its origin in the attempt to set up some law which would be respected and observed upon the seas, where no nation had right of dominion and where lay the free highways of the world. By painful stage after stage has that law been built up, with meager enough results, indeed, after all was accomplished that could be accomplished, but always with a clear view, at least, of what the heart and conscience of mankind demanded.\"\"\"\n",
        "\n",
        "# print(ds.make_notes())"
      ],
      "metadata": {
        "id": "lyl5bl2qat3e"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, List, Optional, Annotated\n",
        "\n",
        "class QuantumNotesState(TypedDict):\n",
        "  file_path: str\n",
        "  task:str\n",
        "  query:Optional[str]\n",
        "  documents: Optional[List[str]]\n",
        "  summary: Optional[str]\n",
        "  notes: Optional[str]\n",
        "  flashcards: Optional[str]\n",
        "  errors: Optional[List[Exception]]\n",
        "  chunks: Optional[List[str]]\n",
        "  embeddings: Optional[List[float]]\n",
        "  vector_store: Optional[str]\n",
        "  context: Optional[str]\n",
        "  model_name: Optional[str]\n",
        "\n"
      ],
      "metadata": {
        "id": "vxJNstm0k9Re"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentProcessingAgent:\n",
        "  def __init__(self):\n",
        "    self.doc_loader = DocumentLoader()\n",
        "    self.text_chunker = TextChunker()\n",
        "    self.embedding_model = EmbeddingModel()\n",
        "    # self.vector_store = VectorStore()\n",
        "\n",
        "\n",
        "  def load_documents_node(self, state:QuantumNotesState):\n",
        "    print(\"In load_docs_node...\")\n",
        "    file_path = state.get('file_path', \"\")\n",
        "    if file_path:\n",
        "      print(f\"Loading documents from {file_path}\")\n",
        "      documents = self.doc_loader.load_document(file_path)\n",
        "      return {\"documents\": documents}\n",
        "    else:\n",
        "      return {\"documents\": []}\n",
        "\n",
        "  def text_chunker_node(self, state:QuantumNotesState):\n",
        "    print(\"In chunking\")\n",
        "    documents = state.get('documents', [])\n",
        "    # chunk_size = state.get('chunk_size', 500)\n",
        "    # chunk_overlap = state.get('chunk_overlap', 50)\n",
        "    if documents:\n",
        "      chunks =  self.text_chunker.chunk_documents(documents)\n",
        "      return {\"chunks\": chunks}\n",
        "    return {\"chunks\": []}\n",
        "\n",
        "  def embeddings_and_vectorstore_node(self, state:QuantumNotesState):\n",
        "    print(\"In embeddings and vectorstore\")\n",
        "    chunks = state.get(\"chunks\", [])\n",
        "    vector_store = VectorStore(self.embedding_model)\n",
        "    if chunks:\n",
        "      print(\"Adding documents to vector store...\")\n",
        "      embeddings = self.embedding_model.embed_docs(chunks)\n",
        "      print(\"After Embeddings..\")\n",
        "      vector_store.add_documents(embeddings, chunks)\n",
        "      return {\"embeddings\": embeddings, \"vector_store\": vector_store}\n",
        "\n",
        "    return {\"embeddings\": [], \"vector_store\":[]}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WyJ9tZ_KmCcq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SummarizingAgent:\n",
        "  def __init__(self, model_name:str=\"llama-3.1-8b-instant\"):\n",
        "    self.model_name = model_name\n",
        "  def make_summary_node(self, state:QuantumNotesState):\n",
        "    print(\"In make_summary\")\n",
        "    vector_store = state.get(\"vector_store\")\n",
        "\n",
        "    ds = DocumentSummarizer(vector_store, self.model_name)\n",
        "    query = state.get(\"query\", \"\")\n",
        "    # query = state.get(\"summary_query\", \"\")\n",
        "    summary = ds.summarize(query)\n",
        "    print(\"Printing summary....\")\n",
        "    return {\"summary\": summary}\n",
        ""
      ],
      "metadata": {
        "id": "qjRd_74Ug3Qp"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NoteMakingAgent:\n",
        "  def __init__(self, model_name:str=\"llama-3.1-8b-instant\"):\n",
        "    self.model_name = model_name\n",
        "\n",
        "  def make_notes_node(self, state:QuantumNotesState):\n",
        "    print(\"In make_notes\")\n",
        "    vector_store = state.get(\"vector_store\")\n",
        "    ds = DocumentSummarizer(vector_store, self.model_name)\n",
        "    query = state.get(\"query\", \"\")\n",
        "    # query = state.get(\"notes_query\", \"\")\n",
        "    notes = ds.make_notes(query)\n",
        "    return {\"notes\": notes}\n"
      ],
      "metadata": {
        "id": "jBz4mDvekWCi"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlashCardMakingAgent:\n",
        "  def __init__(self, model_name:str=\"llama-3.1-8b-instant\"):\n",
        "    self.model_name = model_name\n",
        "\n",
        "  def make_flashcards_node(self, state:QuantumNotesState):\n",
        "    print(\"In make_flashcards\")\n",
        "    vector_store = state.get(\"vector_store\")\n",
        "    ds = DocumentSummarizer(vector_store, self.model_name)\n",
        "    query = state.get(\"query\", \"\")\n",
        "    # query = state.get(\"flash_query\", \"\")\n",
        "    flashcard = ds.create_flashcards(query)\n",
        "    return {\"flashcards\": flashcard}"
      ],
      "metadata": {
        "id": "SZXIWoZClrXO"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class QuantumNotesAgent:\n",
        "  def __init__(self):\n",
        "    self.dpa = DocumentProcessingAgent()\n",
        "    self.sa = SummarizingAgent()\n",
        "    self.nma = NoteMakingAgent()\n",
        "    self.fca = FlashCardMakingAgent()\n",
        "    self.graph = None\n",
        "\n",
        "  def _check_task(self, state:QuantumNotesState):\n",
        "    if state.get(\"task\") == \"summarize\":\n",
        "      return \"make_summary\"\n",
        "    elif state.get(\"task\") == \"make_notes\":\n",
        "      return \"make_notes\"\n",
        "    elif state.get(\"task\") == \"make_flashcards\":\n",
        "      return \"make_flashcards\"\n",
        "    else:\n",
        "      return END # or handle unsupported task\n",
        "\n",
        "  def _build_graph(self):\n",
        "    print(f\"Builder...\")\n",
        "    builder = StateGraph(QuantumNotesState)\n",
        "    builder.add_node(\"load_documents\", self.dpa.load_documents_node)\n",
        "    builder.add_node(\"text_chunker\", self.dpa.text_chunker_node)\n",
        "    builder.add_node(\"embeddings_and_vectorstore\", self.dpa.embeddings_and_vectorstore_node)\n",
        "    builder.add_node(\"make_summary\", self.sa.make_summary_node)\n",
        "    builder.add_node(\"make_notes\", self.nma.make_notes_node)\n",
        "    builder.add_node(\"make_flashcards\", self.fca.make_flashcards_node)\n",
        "\n",
        "\n",
        "    builder.add_edge(START, \"load_documents\")\n",
        "    builder.add_edge(\"load_documents\", \"text_chunker\")\n",
        "    builder.add_edge(\"text_chunker\", \"embeddings_and_vectorstore\")\n",
        "    builder.add_conditional_edges(\"embeddings_and_vectorstore\", self._check_task, [\"make_summary\", \"make_notes\", \"make_flashcards\"])\n",
        "    builder.add_edge(\"make_summary\", END)\n",
        "    builder.add_edge(\"make_notes\", END)\n",
        "    builder.add_edge(\"make_flashcards\", END)\n",
        "    self.graph = builder.compile()\n",
        "\n",
        "  def process_docs(self, state:QuantumNotesState):\n",
        "    if not self.graph:\n",
        "      self._build_graph()\n",
        "    return self.graph.invoke(state)"
      ],
      "metadata": {
        "id": "fSxWspb-E8Y1"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_state = {\n",
        "    \"file_path\": \"/content/speech.txt\",\n",
        "    \"query\": \"what is the speech about?\",\n",
        "    \"task\": \"summarize\"\n",
        "}\n",
        "\n",
        "qa = QuantumNotesAgent()\n",
        "result = qa.process_docs(initial_state)\n",
        "print(result['summary'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka0muNfB_I0F",
        "outputId": "cb91d66b-f274-4bb8-dcd9-50747d11785b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Builder...\n",
            "In load_docs_node...\n",
            "Loading documents from /content/speech.txt\n",
            "In chunking\n",
            "In embeddings and vectorstore\n",
            "Adding documents to vector store...\n",
            "Embedding documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Embeddings..\n",
            "In add documnets, class Vector Store\n",
            "<langchain_community.vectorstores.faiss.FAISS object at 0x791a38ca9160>\n",
            "In make_summary\n",
            "In summarize..\n",
            "In the vector store, similarity search.\n",
            "In embed query with query: what is the speech about?\n",
            ". But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts‚Äîfor democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free\n",
            "Printing summary....\n",
            "The text emphasizes the importance of fighting for fundamental rights and freedoms, including democracy, individual voice in governance, and the rights of small nations. It also highlights the goal of achieving a universal dominion of right through international cooperation among free peoples, with the ultimate aim of bringing peace, safety, and freedom to all nations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_state = {\n",
        "    \"file_path\": \"/content/speech.txt\",\n",
        "    \"query\": \"The text emphasizes the importance of fighting for fundamental rights and freedoms, including democracy, individual voice in governance, and the rights of small nations. It also highlights the goal of achieving a universal dominion of right through international cooperation among free peoples, with the ultimate aim of bringing peace, safety, and freedom to all nations.\",\n",
        "    \"task\": \"make_notes\"\n",
        "}\n",
        "\n",
        "qa = QuantumNotesAgent()\n",
        "result = qa.process_docs(initial_state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UswRP12-ApKa",
        "outputId": "b95ecddf-8a9f-4048-a243-1e16ffe32808"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Builder...\n",
            "In load_docs_node...\n",
            "Loading documents from /content/speech.txt\n",
            "In chunking\n",
            "In embeddings and vectorstore\n",
            "Adding documents to vector store...\n",
            "Embedding documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Embeddings..\n",
            "In add documnets, class Vector Store\n",
            "<langchain_community.vectorstores.faiss.FAISS object at 0x791a3836c7a0>\n",
            "In make_notes\n",
            "In the vector store, similarity search.\n",
            "In embed query with query: The text emphasizes the importance of fighting for fundamental rights and freedoms, including democracy, individual voice in governance, and the rights of small nations. It also highlights the goal of achieving a universal dominion of right through international cooperation among free peoples, with the ultimate aim of bringing peace, safety, and freedom to all nations.\n",
            "<built-in method keys of dict object at 0x791941f06fc0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oM7dEnbmrAy",
        "outputId": "8921eeb8-395f-43e2-d5a9-bacc3a55ccc5"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['file_path', 'task', 'query', 'documents', 'notes', 'chunks', 'embeddings', 'vector_store'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['notes'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3tFl1B6mtYQ",
        "outputId": "01406657-6b37-49da-dafd-a01bb019ea29"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Structured Notes**\n",
            "\n",
            "**Document:** Speech or passage about the importance of democracy and human rights\n",
            "\n",
            "**Key Points:**\n",
            "\n",
            "1. The right is more precious than peace.\n",
            "2. The importance of democracy and human rights.\n",
            "3. The need for universal dominion of right through a concert of free peoples.\n",
            "\n",
            "**Important Details:**\n",
            "\n",
            "1. The right to have a voice in one's own government.\n",
            "2. The rights and liberties of small nations.\n",
            "3. The goal of bringing peace and safety to all nations.\n",
            "4. The ultimate goal of making the world free.\n",
            "\n",
            "**Actionable Insights:**\n",
            "\n",
            "1. **Advocate for democracy:** Support and promote democratic institutions and practices in your community and country.\n",
            "2. **Empower marginalized voices:** Amplify the voices of those who are oppressed or marginalized, and work towards creating a more inclusive and equitable society.\n",
            "3. **Promote international cooperation:** Support international organizations and initiatives that promote peace, safety, and human rights, and work towards creating a global community that values these principles.\n",
            "4. **Educate and raise awareness:** Educate yourself and others about the importance of democracy, human rights, and international cooperation, and work towards raising awareness about these issues in your community and beyond.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['task']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "l1RcR4l-k5sR",
        "outputId": "b413a759-e57d-448c-d0ee-3a205f5ed465"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'summarize'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['query']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "epwkt6ehk_Ac",
        "outputId": "4a23724a-74e9-460d-ab81-cf3c8c924f30"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'what is the speech about?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['summary']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "_q_8QC2qlAX5",
        "outputId": "92e0d769-c52a-45c8-9555-47193572e9ae"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The text emphasizes the importance of fighting for fundamental rights and freedoms, including democracy, individual voice in governance, and the rights of small nations. It also highlights the goal of achieving a universal dominion of right through international cooperation among free peoples, with the ultimate aim of bringing peace, safety, and freedom to all nations.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_state = {\n",
        "    \"file_path\": \"/content/speech.txt\",\n",
        "    \"task\": \"make_notes\"\n",
        "}\n",
        "\n",
        "qa = QuantumNotesAgent()\n",
        "result = qa.process_docs(initial_state)\n",
        "print(result['notes'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-bdiMoOlDc-",
        "outputId": "431d8e98-5e34-49e6-9471-a192d999a95c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Builder...\n",
            "In load_docs_node...\n",
            "Loading documents from /content/speech.txt\n",
            "In chunking\n",
            "In embeddings and vectorstore\n",
            "Adding documents to vector store...\n",
            "Embedding documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Embeddings..\n",
            "In add documnets, class Vector Store\n",
            "<langchain_community.vectorstores.faiss.FAISS object at 0x791a38429c70>\n",
            "In make_notes\n",
            "In the vector store, similarity search.\n",
            "In embed query with query: \n",
            "However, it seems like there is no provided text for me to create notes from. Please provide the text you would like me to analyze and create structured notes from.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_state = {\n",
        "    \"file_path\": \"/content/speech.txt\",\n",
        "    \"query\": \"Sovereignity means supreme power especially over a body politic.\",\n",
        "    \"task\": \"make_flashcards\"\n",
        "}\n",
        "\n",
        "qa = QuantumNotesAgent()\n",
        "result = qa.process_docs(initial_state)\n",
        "print(result['notes'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "LKFsiZGWnDto",
        "outputId": "ad2869ec-1509-4431-dd85-02b1416edbfb"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Builder...\n",
            "In load_docs_node...\n",
            "Loading documents from /content/speech.txt\n",
            "In chunking\n",
            "In embeddings and vectorstore\n",
            "Adding documents to vector store...\n",
            "Embedding documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Embeddings..\n",
            "In add documnets, class Vector Store\n",
            "<langchain_community.vectorstores.faiss.FAISS object at 0x791a382267b0>\n",
            "In make_flashcards\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'notes'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2044212044.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mqa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQuantumNotesAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'notes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'notes'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFqW5bcnndM5",
        "outputId": "a17ac51b-4c6c-47be-e6ce-7d4de5579324"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['file_path', 'task', 'query', 'documents', 'flashcards', 'chunks', 'embeddings', 'vector_store'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['flashcards'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vxtAKrOnidz",
        "outputId": "8ed45b06-0e35-47d6-d4f1-ef36a0178216"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Flashcard:**\n",
            "\n",
            "**Front:** Sovereignty\n",
            "**Back:** Supreme power over a country or government.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t2TXQBg1nl14"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}