# -*- coding: utf-8 -*-
"""SmartLearn AI (QuantumNotes).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X7isEWd9g8dFcibp7ddRTvydh5oDNKjG
"""

!pip install -r requirements.txt

"""API KEY SETUP"""

from google.colab import userdata
import os

# Retrieve and set the API key ONCE at the beginning
groq_api_key = userdata.get('GROQ_API_KEY').strip()
os.environ['GROQ_API_KEY'] = groq_api_key

print(f"✓ GROQ_API_KEY set successfully (length: {len(groq_api_key)} chars)")
print(f"✓ First 10 chars: {groq_api_key[:10]}...")
print(f"✓ Last 10 chars: ...{groq_api_key[-10:]}")

# Verify no newlines or extra characters
if '\n' in groq_api_key or '\r' in groq_api_key:
    print("⚠ WARNING: API key contains newline characters!")
    print("Please check your Colab secrets and remove any extra lines.")
else:
    print("✓ API key format looks good")

import os
import json
import shutil
import pathlib
import gc
from typing import List, Optional, TypedDict
from pathlib import Path

import numpy as np
import pandas as pd
from tqdm import tqdm

# Whisper and audio processing
from faster_whisper import WhisperModel
import ffmpeg

# Text processing
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.tokenize import sent_tokenize

# LangChain and embeddings
from langchain_community.document_loaders import PyMuPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain.vectorstores import Chroma
from sentence_transformers import SentenceTransformer

# LangGraph
from langgraph.graph import StateGraph, START, END

# LLM
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate

"""Directory Setup"""

BASE = "/content"
DATA = f"{BASE}/data"
os.makedirs(f"{DATA}/videos", exist_ok=True)
os.makedirs(f"{DATA}/audio", exist_ok=True)
os.makedirs(f"{DATA}/transcripts", exist_ok=True)
os.makedirs(f"{DATA}/transcripts_raw", exist_ok=True)
os.makedirs(f"{DATA}/chunks", exist_ok=True)

# ========================================
# CONFIGURATION
# ========================================
WHISPER_MODEL = "medium"
VIDEO_EXT = {".mp4", ".mkv", ".mov", ".avi"}
AUDIO_EXT = {".wav", ".mp3", ".m4a", ".flac", ".aac", ".ogg"}
TRANSCRIPT_EXT = {".txt", ".srt", ".vtt", ".json", ".parquet"}
CHUNK_SIZE = 1800  # characters
CHUNK_OVERLAP = 200  # characters

"""File Upload and Routing"""

class FileProcessor:
    """Handles file upload, format detection, and routing"""

    def __init__(self):
        self.whisper_model = None

    def initialize_whisper(self):
        """Initialize Whisper model once"""
        if self.whisper_model is None:
            print("Initializing Whisper model...")
            self.whisper_model = WhisperModel(WHISPER_MODEL, device="cuda", compute_type="float16")
            gc.collect()

    @staticmethod
    def get_file_stem(fname):
        return pathlib.Path(fname).stem

    @staticmethod
    def get_audio_duration(audio_path):
        try:
            probe = ffmpeg.probe(audio_path)
            return float(probe['format']['duration'])
        except ffmpeg.Error as e:
            print(f"Error getting duration: {e}")
            return 0.0

    def extract_audio_from_video(self, video_path, out_audio_path, sr=16000):
        """Convert video to audio"""
        if not os.path.exists(out_audio_path):
            (
                ffmpeg
                .input(video_path)
                .output(out_audio_path, ac=1, ar=sr, format='wav', loglevel="error")
                .overwrite_output()
                .run()
            )
            print(f"Extracted audio: {out_audio_path}")
        return out_audio_path

    def transcribe_audio(self, audio_path, video_id):
        """Transcribe audio using Whisper"""
        self.initialize_whisper()

        segments, info = self.whisper_model.transcribe(
            audio_path,
            beam_size=5,
            vad_filter=True,
            word_timestamps=False
        )

        rows = []
        for i, seg in enumerate(segments):
            rows.append({
                "video_id": video_id,
                "segment_idx": i,
                "start_ts": float(seg.start),
                "end_ts": float(seg.end),
                "text": seg.text.strip()
            })

        out_path = f"{DATA}/transcripts/{video_id}.parquet"
        pd.DataFrame(rows).to_parquet(out_path, index=False)
        print(f"Transcript saved: {out_path}")
        return out_path

    def process_video(self, video_path, video_id):
        """Process video: extract audio, transcribe"""
        audio_path = f"{DATA}/audio/{video_id}.wav"
        self.extract_audio_from_video(video_path, audio_path)

        duration = self.get_audio_duration(audio_path)
        if duration > 7200:  # 2 hours
            print(f"Warning: Audio is {duration/60:.1f} minutes. This may take a while.")

        return self.transcribe_audio(audio_path, video_id)

    def process_audio(self, audio_path, audio_id):
        """Process audio file directly"""
        ext = pathlib.Path(audio_path).suffix.lower()
        dst = f"{DATA}/audio/{audio_id}.wav"

        if ext == ".wav":
            shutil.copy(audio_path, dst)
        else:
            (
                ffmpeg
                .input(audio_path)
                .output(dst, ac=1, ar=16000, format='wav', loglevel="error")
                .overwrite_output()
                .run()
            )

        print(f"Audio ready: {dst}")

        duration = self.get_audio_duration(dst)
        if duration > 7200:
            print(f"Warning: Audio is {duration/60:.1f} minutes.")

        return self.transcribe_audio(dst, audio_id)

    def process_parquet_file(self, parquet_path, doc_id):
        """Process existing parquet transcript file"""
        try:
            df = pd.read_parquet(parquet_path)

            # Validate required columns
            required_cols = ["text"]
            optional_cols = ["video_id", "segment_idx", "start_ts", "end_ts"]

            if "text" not in df.columns:
                raise ValueError("Parquet file must contain a 'text' column")

            # Create a standardized dataframe
            standardized_rows = []

            for idx, row in df.iterrows():
                standardized_row = {
                    "video_id": row.get("video_id", doc_id),
                    "segment_idx": row.get("segment_idx", idx),
                    "start_ts": float(row.get("start_ts", 0.0)),
                    "end_ts": float(row.get("end_ts", 0.0)),
                    "text": str(row["text"]).strip()
                }
                standardized_rows.append(standardized_row)

            # Save to standard location
            std_df = pd.DataFrame(standardized_rows)
            out_path = f"{DATA}/transcripts/{doc_id}.parquet"
            std_df.to_parquet(out_path, index=False)

            print(f"Parquet file processed and saved: {out_path}")
            print(f"Total segments: {len(std_df)}")

            return out_path

        except Exception as e:
            raise ValueError(f"Error processing parquet file: {str(e)}")

    def process_transcript_file(self, transcript_path, doc_id):
        """Process existing transcript files"""
        ext = pathlib.Path(transcript_path).suffix.lower()

        if ext == ".txt":
            with open(transcript_path, 'r', encoding='utf-8', errors='ignore') as f:
                text = f.read().strip()
            rows = [{"segment_idx": 0, "start_ts": 0.0, "end_ts": 0.0, "text": text}]

        elif ext == ".srt":
            subs = pysrt.open(transcript_path, encoding='utf-8')
            rows = [{
                "segment_idx": i,
                "start_ts": s.start.ordinal/1000.0,
                "end_ts": s.end.ordinal/1000.0,
                "text": s.text.replace('\n', ' ').strip()
            } for i, s in enumerate(subs)]

        elif ext == ".vtt":
            def hms_to_s(hms):
                h, m, s = hms.split(':')
                return int(h)*3600 + int(m)*60 + float(s)

            rows = [{
                "segment_idx": i,
                "start_ts": hms_to_s(cap.start),
                "end_ts": hms_to_s(cap.end),
                "text": cap.text.replace('\n', ' ').strip()
            } for i, cap in enumerate(webvtt.read(transcript_path))]

        elif ext == ".json":
            with open(transcript_path, 'r', encoding='utf-8') as f:
                obj = json.load(f)
            segs = obj.get("segments", [])
            rows = [{
                "segment_idx": i,
                "start_ts": float(s.get("start", 0.0)),
                "end_ts": float(s.get("end", 0.0)),
                "text": str(s.get("text", "")).strip()
            } for i, s in enumerate(segs)]

        else:
            raise ValueError(f"Unsupported transcript format: {ext}")

        df = pd.DataFrame(rows)
        df.insert(0, "video_id", doc_id)
        out_path = f"{DATA}/transcripts/{doc_id}.parquet"
        df.to_parquet(out_path, index=False)
        print(f"Transcript saved: {out_path}")
        return out_path

    def process_pdf(self, pdf_path, doc_id):
        """Process PDF file"""
        loader = PyMuPDFLoader(pdf_path)
        docs = loader.load()
        text = " ".join([doc.page_content for doc in docs])

        rows = [{"segment_idx": 0, "start_ts": 0.0, "end_ts": 0.0, "text": text}]
        df = pd.DataFrame(rows)
        df.insert(0, "video_id", doc_id)
        out_path = f"{DATA}/transcripts/{doc_id}.parquet"
        df.to_parquet(out_path, index=False)
        print(f"PDF processed and saved: {out_path}")
        return out_path

    def route_file(self, file_path):
        """Main routing function for any file type"""
        ext = pathlib.Path(file_path).suffix.lower()
        doc_id = self.get_file_stem(file_path)

        if ext in VIDEO_EXT:
            return self.process_video(file_path, doc_id)
        elif ext in AUDIO_EXT:
            return self.process_audio(file_path, doc_id)
        elif ext == ".pdf":
            return self.process_pdf(file_path, doc_id)
        elif ext == ".parquet":
            return self.process_parquet_file(file_path, doc_id)
        elif ext in TRANSCRIPT_EXT:
            return self.process_transcript_file(file_path, doc_id)
        else:
            raise ValueError(f"Unsupported file format: {ext}")

"""Text Chunking"""

class TextChunker:
    """Smart text chunking with sentence awareness and timestamps"""

    @staticmethod
    def normalize_whitespace(text):
        import re
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    @staticmethod
    def basic_cleanup(text):
        text = text.replace("'", "'").replace(""", "\"").replace(""", "\"")
        return TextChunker.normalize_whitespace(text)

    @staticmethod
    def sentence_time_expand(seg_text, seg_start, seg_end):
        """Split segment into sentences with proportional timestamps"""
        txt = TextChunker.basic_cleanup(seg_text)
        sents = [s for s in sent_tokenize(txt) if s.strip()]

        if not sents:
            return []

        total_chars = sum(len(s) for s in sents)
        if total_chars == 0:
            return []

        dur = max(0.0, float(seg_end) - float(seg_start))
        out = []
        cur = float(seg_start)

        for s in sents:
            frac = len(s) / total_chars
            sdur = frac * dur
            out.append({"text": s, "start_ts": cur, "end_ts": cur + sdur})
            cur += sdur

        if out:
            out[-1]["end_ts"] = float(seg_end)

        return out

    @staticmethod
    def build_sentence_table(transcript_parquet_path):
        """Convert transcript to sentence-level dataframe"""
        df = pd.read_parquet(transcript_parquet_path)
        rows = []

        for _, r in df.iterrows():
            exp = TextChunker.sentence_time_expand(r["text"], r["start_ts"], r["end_ts"])
            rows.extend(exp)

        if not rows:
            rows = [{
                "text": TextChunker.basic_cleanup(" ".join(df["text"].tolist())),
                "start_ts": 0.0,
                "end_ts": 0.0
            }]

        return pd.DataFrame(rows)

    @staticmethod
    def make_chunks_from_sentences(sents_df, max_chars=1800, overlap_chars=200):
        """Create overlapping chunks respecting sentence boundaries"""
        chunks = []
        buf_text = ""
        buf_starts = []
        buf_ends = []

        def flush_buffer():
            if not buf_text.strip():
                return
            chunks.append({
                "text": buf_text.strip(),
                "start_ts": min(buf_starts) if buf_starts else 0.0,
                "end_ts": max(buf_ends) if buf_ends else 0.0
            })

        for _, row in sents_df.iterrows():
            s = str(row["text"]).strip()
            st, et = float(row["start_ts"]), float(row["end_ts"])

            if not s:
                continue

            if len(buf_text) + len(s) + 1 <= max_chars:
                buf_text = (buf_text + " " + s).strip() if buf_text else s
                buf_starts.append(st)
                buf_ends.append(et)
            else:
                flush_buffer()
                buf_text = s
                buf_starts = [st]
                buf_ends = [et]

        flush_buffer()
        return chunks

    @staticmethod
    def build_and_save_chunks(transcript_path, max_chars=1800, overlap_chars=200):
        """Main function to create and save chunks"""
        video_id = os.path.splitext(os.path.basename(transcript_path))[0]
        sents_df = TextChunker.build_sentence_table(transcript_path)
        chunks = TextChunker.make_chunks_from_sentences(sents_df, max_chars, overlap_chars)

        out_rows = []
        for i, c in enumerate(chunks):
            out_rows.append({
                "video_id": video_id,
                "chunk_idx": i,
                "start_ts": float(c["start_ts"]),
                "end_ts": float(c["end_ts"]),
                "text": c["text"]
            })

        cdf = pd.DataFrame(out_rows)
        outp = f"{DATA}/chunks/{video_id}_chunks.parquet"
        cdf.to_parquet(outp, index=False)
        print(f"Chunks saved: {outp} | {len(cdf)} chunks")
        return outp

"""Embedding and Vectorstore"""

import uuid
import time
class EmbeddingModel:
    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2'):
        self.model_name = model_name
        self.model = SentenceTransformer(self.model_name)

    def embed_documents(self, texts):
        return self.model.encode(texts, convert_to_tensor=False).tolist()

    def embed_query(self, text):
        return self.model.encode(text, convert_to_tensor=False).tolist()


class VectorStore:
    def __init__(self, embedding_model, persist_dir=None):
        self.embedding_model = embedding_model
        # Create unique directory for each instance to avoid locks
        if persist_dir is None:
            timestamp = int(time.time() * 1000)
            unique_id = str(uuid.uuid4())[:8]
            self.persist_dir = f"/content/chroma_db_{timestamp}_{unique_id}"
        else:
            self.persist_dir = persist_dir
        self.vectorstore = None

    def cleanup(self):
        """Properly cleanup vector store connections and files"""
        if self.vectorstore:
            try:
                # Delete the collection
                if hasattr(self.vectorstore, '_client'):
                    try:
                        self.vectorstore._client.reset()
                    except:
                        pass
                self.vectorstore = None
            except Exception as e:
                print(f"Warning during vectorstore cleanup: {e}")

        # Try to remove the directory
        if os.path.exists(self.persist_dir):
            try:
                import shutil
                shutil.rmtree(self.persist_dir, ignore_errors=True)
                print(f"Removed vector store directory: {self.persist_dir}")
            except Exception as e:
                print(f"Warning: Could not remove directory {self.persist_dir}: {e}")

        # Force garbage collection
        gc.collect()
        time.sleep(0.3)  # Give system time to release locks

    def add_documents(self, documents):
        """Add documents to vector store"""
        print(f"Adding documents to vector store at: {self.persist_dir}")

        # Ensure directory is clean
        if os.path.exists(self.persist_dir):
            try:
                shutil.rmtree(self.persist_dir, ignore_errors=True)
                time.sleep(0.2)
            except Exception as e:
                print(f"Warning clearing directory: {e}")

        # Create new vector store with unique directory
        try:
            self.vectorstore = Chroma.from_documents(
                documents,
                self.embedding_model,
                persist_directory=self.persist_dir
            )
            print(f" Added {len(documents)} documents to vector store")
        except Exception as e:
            print(f"Error creating vector store: {e}")
            # Try without persistence as fallback
            print("Attempting to create in-memory vector store...")
            self.vectorstore = Chroma.from_documents(
                documents,
                self.embedding_model
            )
            print(f" Created in-memory vector store with {len(documents)} documents")

    def similarity_search(self, query, k=25):
        """Search for similar documents"""
        if not self.vectorstore:
            raise ValueError("No documents indexed. Add documents first.")
        return self.vectorstore.similarity_search(query, k=k)

    def load_local(self):
        """Load existing vector store"""
        if os.path.exists(self.persist_dir):
            self.vectorstore = Chroma(
                persist_directory=self.persist_dir,
                embedding_function=self.embedding_model.embed_query
            )
            print(f"Loaded vector store from {self.persist_dir}")
        else:
            print(f"No existing vector store found at {self.persist_dir}")

"""Document Summarization using LLM"""

from google.colab import userdata
import os

class DocumentSummarizer:
    def __init__(self, vector_store, model_name="llama-3.1-8b-instant", groq_api_key=None):
        self.model_name = model_name
        self.vector_store = vector_store

        # Initialize ChatGroq regardless of how the key was provided
        self.chat_groq = ChatGroq(
            model=self.model_name,
            temperature=0.5
        )


    def summarize(self, query, top_k=10):
        """Generate summary based on query"""
        results = self.vector_store.similarity_search(query, k=top_k)
        context = " ".join([doc.page_content for doc in results])

        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are an expert in summarizing documents."),
            ("user",
             "From the following text:\n{context}\n\n"
             "Provide a meaningful and insightful summary related to the query: {query}\n"
             "DO NOT make up information. If the text doesn't contain relevant information, "
             "say 'Information not found in documents related to the query'.")
        ])

        chain = prompt | self.chat_groq
        response = chain.invoke({"context": context, "query": query})
        return response.content

    def make_notes(self, query=None, top_k=10): # Reduced top_k to 10
        """Generate structured notes"""
        if query:
            results = self.vector_store.similarity_search(query, k=top_k)
        else:
            # If no query, retrieve fewer documents to avoid large context
            results = self.vector_store.similarity_search("", k=top_k)


        context = " ".join([doc.page_content for doc in results])

        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are an expert in analyzing documents and creating meaningful notes."),
            ("user",
             "Based on the following text:\n{context}\n\n"
             "Create structured notes with:\n"
             "1. Key Points: [main ideas]\n"
             "2. Important Details: [supporting information]\n"
             "3. Actionable Insights: [what can be done]\n"
             "4. Additional Information: [any other relevant details]")
        ])

        chain = prompt | self.chat_groq
        response = chain.invoke({"context": context})
        return response.content

    def create_flashcards(self, query, top_k=10): # Reduced top_k to 10
        """Generate flashcards"""
        if not query:
            raise ValueError("Query required for flashcard generation")

        results = self.vector_store.similarity_search(query, k=top_k)
        context = " ".join([doc.page_content for doc in results])

        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are an expert in creating educational flashcards."),
            ("user",
             "Based on the following context:\n{context}\n\n"
             "Create flashcards for: {query}\n\n"
             "Each flashcard should have:\n"
             "1. A clear question or prompt (front)\n"
             "2. A concise answer (2-3 lines) capturing key points (back)\n"
             "3. Simple, memorable language\n"
             "Generate 5 flashcards in this format:\n"
             "Flashcard 1:\nFront: [question]\nBack: [answer]\n")
        ])

        chain = prompt | self.chat_groq
        response = chain.invoke({"context": context, "query": query})
        return response.content

"""Langgraph States and Agents"""

class QuantumNotesState(TypedDict):
    file_path: str
    task: str
    query: Optional[str]
    documents: Optional[List[str]]
    summary: Optional[str]
    notes: Optional[str]
    flashcards: Optional[str]
    chunks: Optional[List[str]]
    vector_store: Optional[str]
    transcript_path: Optional[str]
    chunks_path: Optional[str]

class DocumentProcessingAgent:
    """Handles document loading and processing"""

    def __init__(self):
        self.file_processor = FileProcessor()
        self.embedding_model = EmbeddingModel()

    def load_and_transcribe_node(self, state: QuantumNotesState):
        """Node 1: Load file and create transcript"""
        print("=== Step 1: Loading and transcribing ===")
        file_path = state.get('file_path', "")

        if not file_path or not os.path.exists(file_path):
            raise ValueError(f"File not found: {file_path}")

        transcript_path = self.file_processor.route_file(file_path)
        return {"transcript_path": transcript_path}

    def chunking_node(self, state: QuantumNotesState):
        """Node 2: Create chunks from transcript"""
        print("=== Step 2: Creating chunks ===")
        transcript_path = state.get('transcript_path', "")

        if not transcript_path:
            raise ValueError("No transcript path found")

        chunks_path = TextChunker.build_and_save_chunks(transcript_path)
        return {"chunks_path": chunks_path}

    def vectorstore_node(self, state: QuantumNotesState):
        """Node 3: Create embeddings and vector store"""
        print("=== Step 3: Building vector store ===")
        chunks_path = state.get('chunks_path', "")

        if not chunks_path:
            raise ValueError("No chunks path found")

        # Load chunks
        df = pd.read_parquet(chunks_path)
        documents = [
            Document(
                page_content=row["text"],
                metadata={
                    "video_id": row["video_id"],
                    "chunk_idx": row["chunk_idx"],
                    "start_ts": row["start_ts"],
                    "end_ts": row["end_ts"]
                }
            ) for _, row in df.iterrows()
        ]

        # Create NEW vector store with unique directory (no persist_dir argument)
        vector_store = VectorStore(self.embedding_model)
        vector_store.add_documents(documents)

        return {"vector_store": vector_store}

class SummarizingAgent:
    def __init__(self, model_name="llama-3.1-8b-instant"):
        self.model_name = model_name

    def make_summary_node(self, state: QuantumNotesState):
        print("=== Generating Summary ===")
        vector_store = state.get("vector_store")
        query = state.get("query", "")

        # Pass the stored key to DocumentSummarizer
        ds = DocumentSummarizer(vector_store, self.model_name)
        summary = ds.summarize(query)
        return {"summary": summary}


class NoteMakingAgent:
    def __init__(self, model_name="llama-3.1-8b-instant"):
        self.model_name = model_name

    def make_notes_node(self, state: QuantumNotesState):
        print("=== Generating Notes ===")
        vector_store = state.get("vector_store")
        query = state.get("query", "")

        # Pass the stored key to DocumentSummarizer
        ds = DocumentSummarizer(vector_store, self.model_name)
        notes = ds.make_notes(query)
        return {"notes": notes}


class FlashCardMakingAgent:
    def __init__(self, model_name="llama-3.1-8b-instant"):
        self.model_name = model_name


    def make_flashcards_node(self, state: QuantumNotesState):
        print("=== Generating Flashcards ===")
        vector_store = state.get("vector_store")
        query = state.get("query", "")

        # Pass the stored key to DocumentSummarizer
        ds = DocumentSummarizer(vector_store, self.model_name)
        flashcards = ds.create_flashcards(query)
        return {"flashcards": flashcards}

"""Main Pipeline with Langgraph"""

from google.colab import userdata
import os

class SmartLearnPipeline:
    """Main pipeline orchestrating the entire workflow"""

    def __init__(self):
        # Check for API key
        if "GROQ_API_KEY" not in os.environ:
            raise ValueError(
                "GROQ_API_KEY not found in environment. "
                "Please run the setup cell first to set the API key"
            )

        # Initialize agents
        self.dpa = DocumentProcessingAgent()
        self.sa = SummarizingAgent()
        self.nma = NoteMakingAgent()
        self.fca = FlashCardMakingAgent()
        self.graph = None
        self.current_vector_store = None

    def _cleanup_resources(self):
        """Clean up resources before processing new file"""
        if self.current_vector_store:
            print(" Cleaning up previous vector store...")
            self.current_vector_store.cleanup()
            self.current_vector_store = None

        # Additional cleanup: remove any orphaned chroma directories
        try:
            import glob
            old_dirs = glob.glob("/content/chroma_db_*")
            for old_dir in old_dirs:
                try:
                    if os.path.exists(old_dir):
                        shutil.rmtree(old_dir, ignore_errors=True)
                except:
                    pass
        except:
            pass

        # Force garbage collection
        gc.collect()
        time.sleep(0.5)  # Give more time for cleanup

    def _check_task(self, state: QuantumNotesState):
        """Route to appropriate task"""
        task = state.get("task", "").lower()

        if task == "summarize":
            return "make_summary"
        elif task == "make_notes":
            return "make_notes"
        elif task == "make_flashcards" or task == "create_flashcards":
            return "make_flashcards"
        else:
            raise ValueError(f"Unknown task: {task}")

    def _build_graph(self):
        """Build the LangGraph workflow"""
        print("Building workflow graph...")
        builder = StateGraph(QuantumNotesState)

        # Add nodes
        builder.add_node("load_and_transcribe", self.dpa.load_and_transcribe_node)
        builder.add_node("chunking", self.dpa.chunking_node)
        builder.add_node("vectorstore", self.dpa.vectorstore_node)
        builder.add_node("make_summary", self.sa.make_summary_node)
        builder.add_node("make_notes", self.nma.make_notes_node)
        builder.add_node("make_flashcards", self.fca.make_flashcards_node)

        # Define edges
        builder.add_edge(START, "load_and_transcribe")
        builder.add_edge("load_and_transcribe", "chunking")
        builder.add_edge("chunking", "vectorstore")
        builder.add_conditional_edges(
            "vectorstore",
            self._check_task,
            ["make_summary", "make_notes", "make_flashcards"]
        )
        builder.add_edge("make_summary", END)
        builder.add_edge("make_notes", END)
        builder.add_edge("make_flashcards", END)

        self.graph = builder.compile()
        print("Workflow graph built successfully!")

    def process(self, file_path, task, query=None):
        """
        Main entry point for the pipeline

        Args:
            file_path: Path to video/audio/pdf/transcript file
            task: One of 'summarize', 'make_notes', or 'make_flashcards'
            query: Optional query to focus the output

        Returns:
            Dictionary with the requested output
        """

        # Clean up resources from previous run
        self._cleanup_resources()

        if not self.graph:
            self._build_graph()

        initial_state = {
            "file_path": file_path,
            "task": task,
            "query": query or ""
        }

        print(f"\n{'='*60}")
        print(f" Starting SmartLearn Pipeline")
        print(f" File: {os.path.basename(file_path)}")
        print(f" Task: {task}")
        print(f" Query: {query or 'None'}")
        print(f"{'='*60}\n")

        result = self.graph.invoke(initial_state)

        # Store reference to vector store for cleanup
        self.current_vector_store = result.get('vector_store')

        print(f"\n{'='*60}")
        print(" Pipeline completed successfully!")
        print(f"{'='*60}\n")

        return result

    def __del__(self):
        """Cleanup when pipeline is destroyed"""
        self._cleanup_resources()

"""Example Usage"""

pipeline = SmartLearnPipeline()
# Example 1: Process video file and generate notes
result = pipeline.process(
file_path="/content/MCAKCA032-PRINCIPALES OF SOFT COMPUTING-SN SIVNANDAM AND DEEPA SN (1).pdf",
task="make_notes",
query="What is Artificial Neural Network and how does it differ from Biological Neural Network?"
)
print("\n=== NOTES ===")
print(result['notes'])

pipeline = SmartLearnPipeline()
# Example 1: Process video file and generate notes
result = pipeline.process(
file_path="/content/MCAKCA032-PRINCIPALES OF SOFT COMPUTING-SN SIVNANDAM AND DEEPA SN (1).pdf",
task="make_flashcards",
query="Generate flashcards for understanding Linear Vector Quantization"
)
print("\n=== Flashcards ===")
print(result['flashcards'])

pipeline = SmartLearnPipeline()
# Example 1: Process video file and generate notes
result = pipeline.process(
file_path="/content/MCAKCA032-PRINCIPALES OF SOFT COMPUTING-SN SIVNANDAM AND DEEPA SN (1).pdf",
task="summarize",
query="What is Backpropagation Network?"
)
print("\n=== Summary ===")
print(result['summary'])

# Initialize the pipeline without passing the key directly
pipeline = SmartLearnPipeline()

result = pipeline.process(
        file_path="/content/ana-bell-v1.mp3",
        task="summarize",
        query="Summarize the key points"
    )
print("\n=== SUMMARY ===")
print(result['summary'])

# Initialize the pipeline without passing the key directly
pipeline = SmartLearnPipeline()

result = pipeline.process(
        file_path="/content/lecture_transcript.txt",
        task="summarize",
        query="Summarize the key points"
    )
print("\n=== Summarizing the key points ===")
print(result['summary'])

# Initialize the pipeline without passing the key directly
pipeline = SmartLearnPipeline()

result = pipeline.process(
        file_path="/content/ana-bell-v1.parquet",
        task="make_flashcards",
        query="Generating the flashcards"
    )
print("\n=== Flashcards ===")
print(result['flashcards'])

# Initialize the pipeline without passing the key directly
pipeline = SmartLearnPipeline()

result = pipeline.process(
        file_path="/content/ana-bell-v1.parquet",
        task="make_notes",
        query="Generating the notes"
    )
print("\n=== Flashcards ===")
print(result['notes'])

# Initialize the pipeline without passing the key directly
pipeline = SmartLearnPipeline()

result = pipeline.process(
        file_path="/content/MCAKCA032-PRINCIPALES OF SOFT COMPUTING-SN SIVNANDAM AND DEEPA SN (1).pdf",
        task="make_notes",
        query="What is Linear Vector Quantization"
    )
print("\n=== Generating notes ===")
print(result['notes'])

# Initialize the pipeline without passing the key directly
pipeline = SmartLearnPipeline()

result = pipeline.process(
        file_path="/content/MCAKCA032-PRINCIPALES OF SOFT COMPUTING-SN SIVNANDAM AND DEEPA SN (1).pdf",
        task="summarize",
        query="What is the difference between ANN and Biological neural network?"
    )
print("\n=== Generating summary ===")
print(result['summary'])